---
title: "food_desert_analysis_project_2"
author: "Team 1"
date: "`r Sys.Date()`"

output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
editor_options: 
  markdown: 
    wrap: sentence
---

## Data Loading and Preprocessing
```{r init, include=F}
install.packages("caret")
```

```{r init, include=F}
library(tidyverse)
library(ezids)
library(usmap)
library(ModelMetrics)
library(pROC) 
library(ggplot2)
loadPkg("ISLR")
loadPkg("tree") 
library(stats)
loadPkg("rpart")
library(caret)
loadPkg("rattle")

```


```{r setup, include=FALSE}
# Some of common RMD options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
knitr::opts_chunk$set(warning = F, message = F)
# Can globally set option for number display format.
options(scientific=T, digits = 5) 
```


```{r read_data}
# Loading data
data <- read.csv("../Documents/food_access_research_atlas.csv")

```

# Summary of the dataset:

```{r code1.1, echo=TRUE}
str(data)
head(data)
summary(data)
```

# GLM model for food desert

The variable denoting a tract being a food desert is LILATracts_1And10. We initially created a logistic regression model for food desert from the variables we got from the EDA we did in the first project. We found all these variables has significant effect on a tract being a food desert.
Variables : "Urban","GroupQuartersFlag","LowIncomeTracts","lahunv1share","PCTGQTRS","MedianFamilyIncome","lawhite1","lablack1","laasian1","lahisp1","lanhopi1","laomultir1","laaian1","lakids10","lakids1","TractKids","laseniors1","laseniors10","TractKids","TractSeniors","TractWhite","TractBlack","TractAsian","TractNHOPI","TractAIAN","TractOMultir","TractHispanic","TractHUNV","TractSNAP", "PovertyRate"

```{r glm_with_low_income }

model_lila_1_10_var = c("Urban","GroupQuartersFlag","LowIncomeTracts","lahunv1share","PCTGQTRS","MedianFamilyIncome","lawhite1","lablack1","laasian1","lahisp1","lanhopi1","laomultir1","laaian1","lakids10","lakids1","TractKids","laseniors1","laseniors10","TractKids","TractSeniors","TractWhite","TractBlack","TractAsian","TractNHOPI","TractAIAN","TractOMultir","TractHispanic","TractHUNV","TractSNAP", "PovertyRate")
data$State  <- as.factor(data$State)
data$County  <- as.factor(data$County)
data$Urban <- as.factor(data$Urban)
data$GroupQuartersFlag <- as.factor(data$GroupQuartersFlag)
data$LILATracts_1And10 <-  as.factor(data$LILATracts_1And10)
data$LowIncomeTracts <- as.factor(data$LowIncomeTracts)
data$HUNVFlag= as.factor(data$HUNVFlag)
data$LATracts1 = as.factor(data$LATracts1)
data$LATractsVehicle_20 = as.factor(data$LATractsVehicle_20)
data <- ezids::outlierKD2(data, lahunv1share ,qqplt= TRUE, boxplt= TRUE, rm = TRUE)
# Load necessary library

```

```{r logistic_reg_all}
# Create the formula for the glm model
formula_str <- paste("LILATracts_1And10 ~", paste(model_lila_1_10_var, collapse = " + "))

# Fit the GLM model
LILATracts_1And10_logit1 <- glm(as.formula(formula_str), data = data, family = binomial(link = "logit"))


```

```{r logistic_reg_all_analysis_1}
options(max.print = 1e6)
sum_LILATracts_1And10_logit1 = summary(LILATracts_1And10_logit1)
sum_LILATracts_1And10_logit1
#capture.output(sum_LILATracts_1And10_logit1, file = "models/glm_1_logit_lila1_10_summary.txt")
options(max.print = .Options$max.print)
```
The residual deviance decreased by a considerable amount indicating the model is better than a null model

```{r save_model}
saveRDS(LILATracts_1And10_logit1, file = "glm_1_logit_lila1_10.rds")

```

## Summary of the model

```{r check_model}
str(LILATracts_1And10_logit1$fitted.values)
summary(LILATracts_1And10_logit1$fitted.values)
complete_cases <- complete.cases(data[model_lila_1_10_var])
sum(complete_cases)
```

## Removing the nan values

```{r}
sapply(data[model_lila_1_10_var], function(x) sum(is.na(x)))
cleaned_data <- na.omit(data, cols = model_lila_1_10_var)
sapply(cleaned_data[model_lila_1_10_var], function(x) sum(is.na(x)))
```

## Accuracy and confusion matrix  
```{r}

cm = table(cleaned_data$LILATracts_1And10,LILATracts_1And10_logit1$fitted.values>.5)
xkabledply( table(cleaned_data$LILATracts_1And10,LILATracts_1And10_logit1$fitted.values>.5), title = "Confusion matrix from Logit Model" )

precision_stepwise = cm[2,2]/(cm[2,2]+cm[1,2])
precision_stepwise
recall_stepwise = cm[2,2]/(cm[2,2]+cm[2,1])
recall_stepwise
accuracy_stepwise =  (cm[1,1]+cm[2,2])/(cm[1,1]+ cm[1,2]+cm[2,1] + cm[2,2])
accuracy_stepwise



```
The accuracy of the model is `r accuracy_stepwise`.

## Receiver-Operator-Characteristic (ROC) curve and Area-Under-Curve (AUC)


```{r }

cleaned_data$prob=predict(LILATracts_1And10_logit1, type = c("response")) # Add new column of predicted probabilities
h <- roc(LILATracts_1And10~prob, data=cleaned_data)
auc(h)
plot(h)
```




```{r logistic_reg_all_analysis_2}

factors_LILATracts_1And10_logit1 = exp(coef(LILATracts_1And10_logit1))
factors_LILATracts_1And10_logit1
options(max.print = 1e6)
#capture.output(factors_LILATracts_1And10_logit1, file = "models/glm_1_logit_lila1_10_factors.txt")

```

Even though the model gave very good accuracy and AUC is near 1. The ROC curve is also in a perfect shape. We understood all of this indicating some sort of overfitting or perfect separtion. So when we did exponents of the coefficient to find out the odds ratio factors, we found that for every unit increase in LowIncomeTracts1 will increase the odd ratio of being in a food desert by infinity, so we de decided to enquire about it and found out the LILATracts_1And10 is derived variable from LowIncomeTracts1
and that is why there is perfect separation. So we removed the LowIncomeTracts1 variable and then trained models for food desert. Being in urban has a huge impact in a tract being food desert.



## Train/Test Split
We split the data into train and test. Test set has 20% of data. This is to check overfitting and check the performance for unseen data.

```{r}


n <- nrow(cleaned_data)
# Calculate the size of the training set (e.g., 80% of the dataset)
trainSize <- floor(0.8 * n)
# Randomly sample row indices for the training set
trainIndex <- sample(seq_len(n), size = trainSize)
# Create training and test datasets
trainData <- cleaned_data[trainIndex, ]
testData <- cleaned_data[-trainIndex, ]
```

## Stepwise forward

In order to consider only the best variables for a model we used stepwise forward model building by including the previously considered variables but excluding the LowIncomeTracts1.

```{r}
# Load necessary library

model_lila_1_10_var_stepwise = c("Urban","GroupQuartersFlag","lahunv1share","PCTGQTRS","MedianFamilyIncome","lawhite1","lablack1","laasian1","lahisp1","lanhopi1","laomultir1","laaian1","lakids10","lakids1","TractKids","laseniors1","laseniors10","TractKids","TractSeniors","TractWhite","TractBlack","TractAsian","TractNHOPI","TractAIAN","TractOMultir","TractHispanic","TractHUNV","TractSNAP", "PovertyRate")
# Assuming your data is in a dataframe called 'data' and the response variable is 'response'
# Replace 'response' with your actual response variable name
formula_str_2 <- paste("LILATracts_1And10 ~", paste(model_lila_1_10_var_stepwise, collapse = " + "))
# Fit the GLM model
# Initial model with only the intercept (no predictors)
initial_logit2_model <- glm(LILATracts_1And10 ~ 1, data = trainData, family = binomial(link = "logit"))
# Full model with all potential predictors
full_logit2_model <- glm(as.formula(formula_str_2), data = trainData, family = binomial(link = "logit"))
# Perform stepwise forward selection
stepwise_model <- step(initial_logit2_model, scope = list(lower = initial_logit2_model, upper = full_logit2_model), direction = "forward")
# View the summary of the selected model
summary(stepwise_model)
sum_LILATracts_1And10_stepwise_logit2 = summary(stepwise_model)
#capture.output(sum_LILATracts_1And10_stepwise_logit2, file = "models/glm_3_stepwise_logit_lila1_10_wo_lowincome_summary.txt")
#capture.output(factors_LILATracts_1And10_logit2, file = "chiffon_drafts/models/glm_3_stepwise_logit_lila1_10_wo_lowincome_factors.txt")
#saveRDS(stepwise_model, file = "chiffon_drafts/models/rds/glm_3_stepwise_logit_lila1_10_wo_lowincome.rds")
```

The AIC of the intial stepwise model is 34708. But the final AIC of the model is 19854. Which means the final model is actually better. 



```{r}
print(stepwise_model$deviance)
print(stepwise_model$null.deviance)
pchisq(  stepwise_model$null.deviance - stepwise_model$deviance  , 54033 - 54010  , lower.tail = F )

```


