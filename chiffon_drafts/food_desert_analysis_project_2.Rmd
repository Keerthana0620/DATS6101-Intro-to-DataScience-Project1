---
title: "food_desert_analysis_project_2"
author: "Team 1"
date: "`r Sys.Date()`"

output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
editor_options: 
  markdown: 
    wrap: sentence
---

## Data Loading and Preprocessing
```{r init, include=F}
install.packages("caret")
```

```{r init, include=F}
library(tidyverse)
library(ezids)
library(usmap)
library(ModelMetrics)
library(pROC) 
library(ggplot2)
loadPkg("ISLR")
loadPkg("tree") 
library(stats)
loadPkg("rpart")
library(caret)
loadPkg("rattle")

```


```{r setup, include=FALSE}
# Some of common RMD options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
knitr::opts_chunk$set(warning = F, message = F)
# Can globally set option for number display format.
options(scientific=T, digits = 5) 
```


```{r read_data}
# Loading data
data <- read.csv("../Documents/food_access_research_atlas.csv")

```

# Summary of the dataset:

```{r code1.1, echo=TRUE}
str(data)
head(data)
summary(data)
```

# GLM model for food desert

The variable denoting a tract being a food desert is LILATracts_1And10. We initially created a logistic regression model for food desert from the variables we got from the EDA we did in the first project. We found all these variables has significant effect on a tract being a food desert.
Variables : "Urban","GroupQuartersFlag","LowIncomeTracts","lahunv1share","PCTGQTRS","MedianFamilyIncome","lawhite1","lablack1","laasian1","lahisp1","lanhopi1","laomultir1","laaian1","lakids10","lakids1","TractKids","laseniors1","laseniors10","TractKids","TractSeniors","TractWhite","TractBlack","TractAsian","TractNHOPI","TractAIAN","TractOMultir","TractHispanic","TractHUNV","TractSNAP", "PovertyRate"

```{r glm_with_low_income }

model_lila_1_10_var = c("Urban","GroupQuartersFlag","LowIncomeTracts","lahunv1share","PCTGQTRS","MedianFamilyIncome","lawhite1","lablack1","laasian1","lahisp1","lanhopi1","laomultir1","laaian1","lakids10","lakids1","TractKids","laseniors1","laseniors10","TractKids","TractSeniors","TractWhite","TractBlack","TractAsian","TractNHOPI","TractAIAN","TractOMultir","TractHispanic","TractHUNV","TractSNAP", "PovertyRate")
data$State  <- as.factor(data$State)
data$County  <- as.factor(data$County)
data$Urban <- as.factor(data$Urban)
data$GroupQuartersFlag <- as.factor(data$GroupQuartersFlag)
data$LILATracts_1And10 <-  as.factor(data$LILATracts_1And10)
data$LowIncomeTracts <- as.factor(data$LowIncomeTracts)
data$HUNVFlag= as.factor(data$HUNVFlag)
data$LATracts1 = as.factor(data$LATracts1)
data$LATractsVehicle_20 = as.factor(data$LATractsVehicle_20)
data <- ezids::outlierKD2(data, lahunv1share ,qqplt= TRUE, boxplt= TRUE, rm = TRUE)
# Load necessary library

```

```{r logistic_reg_all}
# Create the formula for the glm model
formula_str <- paste("LILATracts_1And10 ~", paste(model_lila_1_10_var, collapse = " + "))

# Fit the GLM model
LILATracts_1And10_logit1 <- glm(as.formula(formula_str), data = data, family = binomial(link = "logit"))


```

```{r logistic_reg_all_analysis_1}
options(max.print = 1e6)
sum_LILATracts_1And10_logit1 = summary(LILATracts_1And10_logit1)
sum_LILATracts_1And10_logit1
#capture.output(sum_LILATracts_1And10_logit1, file = "models/glm_1_logit_lila1_10_summary.txt")
options(max.print = .Options$max.print)
```
The residual deviance decreased by a considerable amount indicating the model is better than a null model

```{r save_model}
saveRDS(LILATracts_1And10_logit1, file = "glm_1_logit_lila1_10.rds")

```

## Summary of the model

```{r check_model}
str(LILATracts_1And10_logit1$fitted.values)
summary(LILATracts_1And10_logit1$fitted.values)
complete_cases <- complete.cases(data[model_lila_1_10_var])
sum(complete_cases)
```

## Removing the nan values

```{r}
sapply(data[model_lila_1_10_var], function(x) sum(is.na(x)))
cleaned_data <- na.omit(data, cols = model_lila_1_10_var)
sapply(cleaned_data[model_lila_1_10_var], function(x) sum(is.na(x)))
```

## Accuracy and confusion matrix  
```{r}

cm = table(cleaned_data$LILATracts_1And10,LILATracts_1And10_logit1$fitted.values>.5)
xkabledply( table(cleaned_data$LILATracts_1And10,LILATracts_1And10_logit1$fitted.values>.5), title = "Confusion matrix from Logit Model" )

precision_stepwise = cm[2,2]/(cm[2,2]+cm[1,2])
precision_stepwise
recall_stepwise = cm[2,2]/(cm[2,2]+cm[2,1])
recall_stepwise
accuracy_stepwise =  (cm[1,1]+cm[2,2])/(cm[1,1]+ cm[1,2]+cm[2,1] + cm[2,2])
accuracy_stepwise



```
The accuracy of the model is `r accuracy_stepwise`.

## Receiver-Operator-Characteristic (ROC) curve and Area-Under-Curve (AUC)


```{r }

cleaned_data$prob=predict(LILATracts_1And10_logit1, type = c("response")) # Add new column of predicted probabilities
h <- roc(LILATracts_1And10~prob, data=cleaned_data)
auc(h)
plot(h)
```




```{r logistic_reg_all_analysis_2}

factors_LILATracts_1And10_logit1 = exp(coef(LILATracts_1And10_logit1))
factors_LILATracts_1And10_logit1
options(max.print = 1e6)
#capture.output(factors_LILATracts_1And10_logit1, file = "models/glm_1_logit_lila1_10_factors.txt")

```

Even though the model gave very good accuracy and AUC is near 1. The ROC curve is also in a perfect shape. We understood all of this indicating some sort of overfitting or perfect separtion. So when we did exponents of the coefficient to find out the odds ratio factors, we found that for every unit increase in LowIncomeTracts1 will increase the odd ratio of being in a food desert by infinity, so we de decided to enquire about it and found out the LILATracts_1And10 is derived variable from LowIncomeTracts1
and that is why there is perfect separation. So we removed the LowIncomeTracts1 variable and then trained models for food desert. Being in urban has a huge impact in a tract being food desert.



## Train/Test Split
We split the data into train and test. Test set has 20% of data. This is to check overfitting and check the performance for unseen data.

```{r}


n <- nrow(cleaned_data)
# Calculate the size of the training set (e.g., 80% of the dataset)
trainSize <- floor(0.8 * n)
# Randomly sample row indices for the training set
trainIndex <- sample(seq_len(n), size = trainSize)
# Create training and test datasets
trainData <- cleaned_data[trainIndex, ]
testData <- cleaned_data[-trainIndex, ]
```


